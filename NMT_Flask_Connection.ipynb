{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalFlaskNMT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chitrasingh98/-IntersectingArea-/blob/master/NMT_Flask_Connection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HPDpMSpSEz9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOqgIXKgDjJP"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYqrxfXiRG2E"
      },
      "source": [
        "cd /content/gdrive/My Drive/NMT_Marathi2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVDZ22N4F4_-"
      },
      "source": [
        "from attention import AttentionLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBhX5TnWO2Xr"
      },
      "source": [
        "\n",
        "from flask import Flask, request, jsonify, render_template\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "from string import digits\n",
        "import re\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import LSTM, Input, Dense,Embedding, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model,load_model, model_from_json\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pickle as pkl\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENT-k_uPRtxK"
      },
      "source": [
        "# loading the model architecture and asigning the weights\n",
        "json_file = open('NMT_model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model_loaded = model_from_json(loaded_model_json, custom_objects={'AttentionLayer': AttentionLayer})\n",
        "# load weights into new model\n",
        "model_loaded.load_weights(\"NMT_model_weight.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5upjUHyaSEsU"
      },
      "source": [
        "with open('NMT_Etokenizer.pkl','rb') as f:\n",
        "  vocab_size_source, Eword2index, englishTokenizer = pkl.load(f)\n",
        "\n",
        "with open('NMT_Mtokenizer.pkl', 'rb') as f:\n",
        "  vocab_size_target, Mword2index, marathiTokenizer = pkl.load(f)\n",
        "\n",
        "with open('NMT_data.pkl','rb') as f:\n",
        "  X_train, y_train, X_test, y_test = pkl.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VIhrLXeSWbV"
      },
      "source": [
        "Eindex2word = englishTokenizer.index_word\n",
        "Mindex2word = marathiTokenizer.index_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXZ5bLw0NP7j"
      },
      "source": [
        "def remove_punc(text_list):\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  removed_punc_text = []\n",
        "  for sent in text_list:\n",
        "    sentance = [w.translate(table) for w in sent.split(' ')]\n",
        "    removed_punc_text.append(' '.join(sentance))\n",
        "  return removed_punc_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWSBcxreNVWI"
      },
      "source": [
        "def Max_length(data):\n",
        "  max_length_ = max([len(x.split(' ')) for x in data])\n",
        "  return max_length_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul23Y31BMrdY"
      },
      "source": [
        "def preprocessing(userInput):\n",
        "  # userInput=[\"how are you\"]\n",
        "  userInput= [x.lower() for x in userInput]\n",
        "  userInput= [re.sub(\"'\",'',x) for x in userInput]\n",
        "  # remove punctuation\n",
        "\n",
        "  userInput= remove_punc(userInput)\n",
        "  # remove digits\n",
        "  remove_digits = str.maketrans('', '', digits)\n",
        "  removed_digits_text = []\n",
        "  for sent in userInput:\n",
        "    sentance = [w.translate(remove_digits) for w in sent.split(' ')]\n",
        "    removed_digits_text.append(' '.join(sentance))\n",
        "  userInput = removed_digits_text\n",
        "\n",
        "  # removing the stating and ending whitespaces\n",
        "  userInput = [x.strip() for x in userInput]\n",
        "\n",
        "\n",
        "  # Finding max length of input data\n",
        "  max_length_user_input = Max_length(userInput)\n",
        "\n",
        "\n",
        "  # Eword2index = englishTokenizer.word_index\n",
        "  vocab_size_source = len(Eword2index) + 1\n",
        "  max_length_english=11\n",
        "\n",
        "  userInput = englishTokenizer.texts_to_sequences(userInput)\n",
        "  userInput = pad_sequences(userInput, maxlen=max_length_english, padding='post')\n",
        "  print(userInput)\n",
        "  return userInput\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkquFI06gMQ0"
      },
      "source": [
        "model_loaded.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93UwmPxwSg3s"
      },
      "source": [
        "latent_dim=500\n",
        "# encoder inference\n",
        "encoder_inputs = model_loaded.input[0]  #loading encoder_inputs\n",
        "encoder_outputs, state_h, state_c = model_loaded.layers[6].output #loading encoder_outputs\n",
        "\n",
        "print(encoder_outputs.shape)\n",
        "\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# decoder inference\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(11,latent_dim))\n",
        "decoder_states_inputs = [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "decoder_inputs = model_loaded.layers[3].output\n",
        "\n",
        "print(decoder_inputs.shape)\n",
        "dec_emb_layer = model_loaded.layers[5]\n",
        "\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_lstm = model_loaded.layers[7]\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "decoder_states = [state_h2, state_c2]\n",
        "\n",
        "#attention inference\n",
        "attn_layer = model_loaded.layers[8]\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "\n",
        "concate = model_loaded.layers[9]\n",
        "decoder_inf_concat = concate([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_dense = model_loaded.layers[10]\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "[decoder_outputs2] + [state_h2, state_c2])\n",
        "# decoder_model = Model(inputs=[decoder_inputs].append(decoder_states_inputs), outputs=[decoder_outputs2].append(decoder_states))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjJnpHaLSraf"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Chose the 'start' word as the first word of the target sequence\n",
        "    target_seq[0, 0] = Mword2index['start']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        if sampled_token_index == 0:\n",
        "          break\n",
        "        else:\n",
        "          sampled_token = Mindex2word[sampled_token_index]\n",
        "\n",
        "          if(sampled_token!='end'):\n",
        "              decoded_sentence += ' '+sampled_token\n",
        "\n",
        "              # Exit condition: either hit max length or find stop word.\n",
        "              if (sampled_token == 'end' or len(decoded_sentence.split()) >= (26-1)):\n",
        "                  stop_condition = True\n",
        "\n",
        "          # Update the target sequence (of length 1).\n",
        "          target_seq = np.zeros((1,1))\n",
        "          target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "          # Update internal states\n",
        "          e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOaWp74qTMl8"
      },
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if((i!=0 and i!=Mword2index['start']) and i!=Mword2index['end']):\n",
        "        newString=newString+Mindex2word[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if(i!=0):\n",
        "        newString=newString+Eindex2word[i]+' '\n",
        "    return newString"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzJkBArQTnZX"
      },
      "source": [
        "def translate(userInp):\n",
        "  userInput=[]\n",
        "\n",
        "  userInput.append(userInp)\n",
        "  userInput=preprocessing(userInput) \n",
        "  print(\"Review:\",seq2text(userInput[0]))\n",
        "  strMar=decode_sequence(userInput[0].reshape(1,11))\n",
        "  print(\"Predicted summary:\",strMar)\n",
        "  return strMar\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9yU3I5EzAf1"
      },
      "source": [
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSQQcV4ozMaj",
        "outputId": "b4e7782d-70b4-484e-d83e-40dee4479d3c"
      },
      "source": [
        "\n",
        "from flask import Flask, render_template, request\n",
        "\n",
        "app = Flask(__name__, template_folder='/content/gdrive/MyDrive/NMT_Marathi2/templates')\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template('frontend.html')\n",
        "\n",
        "@app.route('/predict',methods=['POST'])\n",
        "def predict():\n",
        "    '''\n",
        "    For rendering results on HTML GUI\n",
        "    '''\n",
        "    englishSentence = [str(x) for x in request.form.values()]\n",
        "    # printTest(englishSentence)\n",
        "    # js=j(englishSentence[0])\n",
        "    js=translate(englishSentence[0])\n",
        "    output=englishSentence[0].upper()\n",
        "    return render_template('frontend.html',original_text='Original English Text:  {}'.format(englishSentence[0]), prediction_text='Translated Text:  {}'.format(js))\n",
        "\n",
        "\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
            "127.0.0.1 - - [10/Feb/2021 13:46:17] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [10/Feb/2021 13:46:18] \"\u001b[33mGET /styling.css HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [10/Feb/2021 13:46:19] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[3587    0    0    0    0    0    0    0    0    0    0]]\n",
            "Review: hello \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [10/Feb/2021 13:46:23] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted summary:  हॅलो\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [10/Feb/2021 13:46:24] \"\u001b[33mGET /styling.css HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [10/Feb/2021 13:46:24] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[ 21  15   3 190   0   0   0   0   0   0   0]]\n",
            "Review: what are you doing \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [10/Feb/2021 14:05:34] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted summary:  तू काय म्हणत आहेस\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [10/Feb/2021 14:05:35] \"\u001b[33mGET /styling.css HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [10/Feb/2021 14:05:35] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}